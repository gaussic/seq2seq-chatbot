{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from masked_cross_entropy import masked_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'data/twitter'\n",
    "train_dir = os.path.join(base_dir, 'twitter_clean.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'twitter_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \"\"\"\n",
    "    ÊñáÊú¨È¢ÑÂ§ÑÁêÜÔºåËé∑ÂèñËØçÊ±áË°®ÔºåÂπ∂Â∞ÜÂ≠óÁ¨¶‰∏≤ÊñáÊú¨ËΩ¨Êç¢‰∏∫Êï∞Â≠óÂ∫èÂàó„ÄÇ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_dir, vocab_dir):\n",
    "        assert os.path.exists(train_dir), 'File %s does not exist.' % train_dir\n",
    "        assert os.path.exists(vocab_dir), 'File %s does not exist.' % vocab_dir\n",
    "\n",
    "        words = open(vocab_dir, encoding='utf-8').read().strip().split('\\n')\n",
    "        word_to_id = dict(zip(words, range(len(words))))\n",
    "        \n",
    "        assert word_to_id['<pad>'] == 0, \"<pad> id should be 0.\"\n",
    "        \n",
    "        self.words = words\n",
    "        self.word_to_id = word_to_id\n",
    "        \n",
    "        self.tokenize(train_dir)\n",
    "        \n",
    "    def tokenize(self, train_dir):\n",
    "        data = open(train_dir, encoding='utf-8').read().strip().split('\\n')\n",
    "        questions, answers = [], []\n",
    "        for line in data:\n",
    "            question, answer = line.split(\" ==> \")\n",
    "            questions.append(self.text_to_ids(question))\n",
    "            answers.append(self.text_to_ids(answer))\n",
    "            \n",
    "        total_num = len(questions)\n",
    "        train_num = int(0.9 * total_num)\n",
    "        self.x_train, self.y_train = questions[:train_num], answers[:train_num]\n",
    "        self.x_test, self.y_test = questions[train_num:], answers[train_num:]\n",
    "        \n",
    "    def text_to_ids(self, text):\n",
    "        return [self.word_to_id[x] for x in (text.split() + ['<eos>'])]\n",
    "    \n",
    "    def ids_to_text(self, ids):\n",
    "        return [self.words[x] for x in ids]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Train length: %d, Test length: %d, Vocabulary size: %d\" % (len(self.x_train), \n",
    "                                                                           len(self.x_test), \n",
    "                                                                           len(self.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, data, labels, batch_size=64):\n",
    "        num_batches = len(data) // batch_size\n",
    "        data, labels = data[:(num_batches * batch_size)], labels[:(num_batches * batch_size)]\n",
    "        self.data = []\n",
    "        for i in range(num_batches):\n",
    "            x_batch = data[(i * batch_size):((i+1)*batch_size)]\n",
    "            y_batch = labels[(i * batch_size):((i+1)*batch_size)]\n",
    "            x_pad, x_lengths, y_pad, y_lengths = self.pad_batch(x_batch, y_batch)\n",
    "            self.data.append((x_pad, x_lengths, y_pad, y_lengths))\n",
    "            \n",
    "    def pad_batch(self, x_batch, y_batch):\n",
    "        seq_pairs = sorted(zip(x_batch, y_batch), key=lambda p: len(p[0]), reverse=True)\n",
    "        x_batch, y_batch = zip(*seq_pairs)\n",
    "        \n",
    "        x_lengths = list(map(len, x_batch))\n",
    "        x_padded = [self.pad_seq(s, max(x_lengths)) for s in x_batch]\n",
    "        \n",
    "        y_lengths = list(map(len, y_batch))\n",
    "        y_padded = [self.pad_seq(s, max(y_lengths)) for s in y_batch]\n",
    "        \n",
    "        input_var = Variable(torch.LongTensor(x_padded)).transpose(0, 1)\n",
    "        target_var = Variable(torch.LongTensor(y_padded)).transpose(0, 1)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_var = input_var.cuda()\n",
    "            target_var = target_var.cuda()\n",
    "            \n",
    "        return input_var, x_lengths, target_var, y_lengths\n",
    "    \n",
    "    def pad_seq(self, seq, max_len):\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embedding, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden):\n",
    "        # Note: we run this one step at a time\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(rnn_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([corpus.word_to_id['<sos>']] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths,\n",
    "        USE_CUDA\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq, max_length=20):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor([input_seq]), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([corpus.word_to_id['<sos>']]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden\n",
    "        )\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == corpus.word_to_id['<eos>']:\n",
    "            decoded_words.append('<eos>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(corpus.words[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    index = random.choice(range(len(corpus.x_test)))\n",
    "    input_seq, target_seq = corpus.x_test[index], corpus.y_test[index]\n",
    "    output_words = evaluate(input_seq)\n",
    "    print(\"Input:\", ' '.join(corpus.ids_to_text(input_seq)))\n",
    "    print(\"Target:\", ' '.join(corpus.ids_to_text(target_seq)))\n",
    "    print(\"Predicted:\", ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train length: 194785, Test length: 21643, Vocabulary size: 10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(train_dir, vocab_dir)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 100\n",
    "\n",
    "train_data = DataSet(corpus.x_train, corpus.y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 50000\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 500\n",
    "evaluate_every = 2000\n",
    "\n",
    "vocab_size = len(corpus.words)\n",
    "embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(hidden_size, embedding, n_layers, dropout=dropout)\n",
    "decoder = DecoderRNN(hidden_size, vocab_size, embedding, n_layers, dropout=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 25s (- 239m 58s) (500 1%) 5.9261\n",
      "4m 9s (- 204m 5s) (1000 2%) 5.3262\n",
      "5m 54s (- 190m 53s) (1500 3%) 5.0771\n",
      "7m 39s (- 183m 46s) (2000 4%) 4.9334\n",
      "Input: watching friday night lights , <unk> hella socks lmaooo . like a house wife üíÅ üèΩ <eos>\n",
      "Target: this show is fucking stressful <eos>\n",
      "Predicted: i was just thinking about it <eos>\n",
      "Input: almost a year with joe :) ) <eos>\n",
      "Target: i'm so excited ! ! ! <eos>\n",
      "Predicted: i was thinking about it . <eos>\n",
      "9m 27s (- 179m 38s) (2500 5%) 4.8400\n",
      "11m 12s (- 175m 38s) (3000 6%) 4.7588\n",
      "12m 57s (- 172m 13s) (3500 7%) 4.7014\n",
      "14m 43s (- 169m 14s) (4000 8%) 4.6281\n",
      "Input: <unk> vodka is the worst thing ever created <eos>\n",
      "Target: i fucked with it <eos>\n",
      "Predicted: i know , i think it's the <unk> of the <unk> <eos>\n",
      "Input: <unk> : she really meant <unk> <unk> <eos>\n",
      "Target: he <unk> his first name differently <eos>\n",
      "Predicted: she is a <unk> <eos>\n",
      "16m 28s (- 166m 33s) (4500 9%) 4.5872\n",
      "18m 13s (- 163m 59s) (5000 10%) 4.5265\n",
      "19m 58s (- 161m 38s) (5500 11%) 4.5012\n",
      "21m 43s (- 159m 21s) (6000 12%) 4.4412\n",
      "Input: i used to think orlando <unk> was the <unk> man ever but now his nose pisses me off <eos>\n",
      "Target: yeah he pisses me off . he let <unk> <unk> get engaged to the snapchat ceo üôÑ <eos>\n",
      "Predicted: i was just thinking about this . <eos>\n",
      "Input: <unk> where did you see it ? looking forward to see it as well . big fan from nyc ! <eos>\n",
      "Target: they sent us a <unk> . <eos>\n",
      "Predicted: thanks for the support ! <eos>\n",
      "23m 28s (- 157m 8s) (6500 13%) 4.4121\n",
      "25m 14s (- 155m 1s) (7000 14%) 4.3545\n",
      "26m 59s (- 152m 57s) (7500 15%) 4.3365\n",
      "28m 44s (- 150m 54s) (8000 16%) 4.2889\n",
      "Input: much innovation ! <unk> ( we really need a <unk> emoji ) <eos>\n",
      "Target: very <unk> . such <unk> fools . <eos>\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Input: first passengers trailer has charm for days . <eos>\n",
      "Target: really ? <unk> , this thing looks like hot trash . <eos>\n",
      "Predicted: i hope you enjoyed it ! <eos>\n",
      "30m 29s (- 148m 54s) (8500 17%) 4.2886\n",
      "32m 15s (- 146m 55s) (9000 18%) 4.2530\n",
      "34m 0s (- 144m 59s) (9500 19%) 4.2191\n",
      "35m 45s (- 143m 2s) (10000 20%) 4.1633\n",
      "Input: happy ! by comes out in 3 more weeks ! <eos>\n",
      "Target: any idea why it isn ‚Äô t on yet ? <eos>\n",
      "Predicted: thank you ! ! ! <eos>\n",
      "Input: apple hater <eos>\n",
      "Target: god forbid i say anything against apple :) <eos>\n",
      "Predicted: i don't know what to say <eos>\n",
      "37m 30s (- 141m 7s) (10500 21%) 4.1491\n",
      "39m 15s (- 139m 12s) (11000 22%) 4.1170\n",
      "41m 1s (- 137m 19s) (11500 23%) 4.0892\n",
      "42m 46s (- 135m 25s) (12000 24%) 4.0648\n",
      "Input: he gotta few hit but i never heard the album either <eos>\n",
      "Target: i don't kno no other singles but \" 2 phones \" lol . <eos>\n",
      "Predicted: i know right ? <eos>\n",
      "Input: dems lost their <unk> after the <unk> and had to push through <unk> frank . republicans need 51 votes . <eos>\n",
      "Target: that's the point of <unk> article and mine . <eos>\n",
      "Predicted: i don't think they are all about the <unk> of the <unk> . <eos>\n",
      "44m 31s (- 133m 34s) (12500 25%) 4.0753\n",
      "46m 16s (- 131m 42s) (13000 26%) 4.0078\n",
      "48m 1s (- 129m 51s) (13500 27%) 3.9962\n",
      "49m 47s (- 128m 1s) (14000 28%) 3.9466\n",
      "Input: that's a good question ! let's ask üç© üíî <eos>\n",
      "Target: no just beer and weed and hot dogs ! <eos>\n",
      "Predicted: i know , i just want to go to the gym . <eos>\n",
      "Input: next time you are going la let me know ! <eos>\n",
      "Target: tomorrow . come . <eos>\n",
      "Predicted: i will ! ! ! <eos>\n",
      "51m 32s (- 126m 11s) (14500 28%) 3.9699\n",
      "53m 18s (- 124m 22s) (15000 30%) 3.9311\n",
      "55m 3s (- 122m 32s) (15500 31%) 3.8947\n",
      "56m 48s (- 120m 42s) (16000 32%) 3.9040\n",
      "Input: last i saw section <unk> was like 200 a ticket <eos>\n",
      "Target: i think it's like $ <unk> <eos>\n",
      "Predicted: i was <unk> in the middle of the day <eos>\n",
      "Input: though fortunately to share exact location you do have to do it <unk> on a per tweet basis . <eos>\n",
      "Target: but doesn't this show people your location ? i don't want people tracking me down . <eos>\n",
      "Predicted: i don't know if i can use it . <eos>\n",
      "58m 33s (- 118m 53s) (16500 33%) 3.8484\n",
      "60m 18s (- 117m 4s) (17000 34%) 3.8394\n",
      "62m 3s (- 115m 15s) (17500 35%) 3.7865\n",
      "63m 48s (- 113m 26s) (18000 36%) 3.8203\n",
      "Input: they'd rather watch americans die at the hands of muslims than admit they're living a lie about islam . <eos>\n",
      "Target: at their <unk> , that's a big gay area , picked up votes there ! <eos>\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Input: soooooo pissed right now <eos>\n",
      "Target: was it a chick ? <eos>\n",
      "Predicted: i know right ? <eos>\n",
      "65m 34s (- 111m 38s) (18500 37%) 3.7806\n",
      "67m 19s (- 109m 50s) (19000 38%) 3.7650\n",
      "69m 4s (- 108m 1s) (19500 39%) 3.7902\n",
      "70m 48s (- 106m 13s) (20000 40%) 3.7151\n",
      "Input: even tho i said it in person already i'll say it again on twitter happy birthday üéâ üéâ üéâ <eos>\n",
      "Target: omg i love you üòÇ ‚ù§ Ô∏è <eos>\n",
      "Predicted: thank you ! ! ! <eos>\n",
      "Input: yeah there's a huge accident üòû <eos>\n",
      "Target: i saw when i finally got up to the light :-( <eos>\n",
      "Predicted: i think you mean the <unk> <eos>\n",
      "72m 33s (- 104m 25s) (20500 41%) 3.7191\n",
      "74m 18s (- 102m 37s) (21000 42%) 3.6765\n",
      "76m 3s (- 100m 49s) (21500 43%) 3.6645\n",
      "77m 48s (- 99m 1s) (22000 44%) 3.6215\n",
      "Input: i hope you laugh at them as much as i do because i was dying last night üòÇ üòÇ üòÇ <eos>\n",
      "Target: absolutely die laughing üòÇ üòÇ üòÇ <eos>\n",
      "Predicted: i was just sitting there in a few weeks ago üòÇ üòÇ üòÇ <eos>\n",
      "Input: wtf is a muslim ‚Äò free <unk> ‚Äô ? <eos>\n",
      "Target: i asked her if she <unk> koran <unk> for saying islam is perfect and she ran . <eos>\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "79m 33s (- 97m 14s) (22500 45%) 3.6476\n",
      "81m 18s (- 95m 26s) (23000 46%) 3.6265\n",
      "83m 3s (- 93m 39s) (23500 47%) 3.6068\n",
      "84m 48s (- 91m 52s) (24000 48%) 3.5765\n",
      "Input: the world's most dangerous morning show is on click here to stream us live ! ! ! <eos>\n",
      "Target: u know yo who u talking to like that dude ill drake ur ass up <eos>\n",
      "Predicted: so excited for this video ! ! ! <eos>\n",
      "Input: yikes he's a cop <eos>\n",
      "Target: yikes it's all the brothers fault <eos>\n",
      "Predicted: i know . i just got a copy of the <unk> . <eos>\n",
      "86m 33s (- 90m 5s) (24500 49%) 3.5581\n",
      "88m 18s (- 88m 18s) (25000 50%) 3.5870\n",
      "90m 3s (- 86m 31s) (25500 51%) 3.5942\n",
      "91m 48s (- 84m 44s) (26000 52%) 3.5051\n",
      "Input: where do i go for drag race tn ? my date cancelled on me <eos>\n",
      "Target: i was just told to go to tool box lol <eos>\n",
      "Predicted: i was about to tweet this <eos>\n",
      "Input: did you take the pic ? if so , from where ? <eos>\n",
      "Target: i did ! ! ! from the <unk> deck ! just on my <eos>\n",
      "Predicted: yes ! ! ! i was just trying to do a <unk> before i was late to the party <eos>\n",
      "93m 33s (- 82m 57s) (26500 53%) 3.5193\n",
      "95m 18s (- 81m 11s) (27000 54%) 3.4931\n",
      "97m 3s (- 79m 24s) (27500 55%) 3.4742\n",
      "98m 48s (- 77m 38s) (28000 56%) 3.4650\n",
      "Input: 65 degrees in nyc today and every girl is dressed like a sand person . <eos>\n",
      "Target: how am i so cold already <eos>\n",
      "Predicted: this is a <unk> thing <eos>\n",
      "Input: do i have to read art of war to figure out your strategy ! ? <eos>\n",
      "Target: there isn't that i have seen work . many hours of study <eos>\n",
      "Predicted: i think you mean <unk> is a <unk> for <unk> . <eos>\n",
      "100m 33s (- 75m 51s) (28500 56%) 3.4648\n",
      "102m 18s (- 74m 4s) (29000 57%) 3.4297\n",
      "104m 2s (- 72m 18s) (29500 59%) 3.4324\n",
      "105m 47s (- 70m 31s) (30000 60%) 3.3999\n",
      "Input: nice ! are you using mix and where can i play with the <eos>\n",
      "Target: not using mix ! stay tuned üòú <eos>\n",
      "Predicted: <unk> <unk> ! ! ! <eos>\n",
      "Input: do i go and sell all my xbox 360 shit today or no <eos>\n",
      "Target: get xbox 1 so i can give you the work bro <eos>\n",
      "Predicted: i feel like this is a fact <eos>\n",
      "107m 32s (- 68m 45s) (30500 61%) 3.3613\n",
      "109m 17s (- 66m 59s) (31000 62%) 3.3875\n",
      "111m 2s (- 65m 12s) (31500 63%) 3.3537\n",
      "112m 47s (- 63m 26s) (32000 64%) 3.3417\n",
      "Input: university of hawaii at <unk> :: <unk> , hi <eos>\n",
      "Target: your school <eos>\n",
      "Predicted: <unk> <unk> <eos>\n",
      "Input: lmfao he's always on point <eos>\n",
      "Target: lmfaoooo all the time with the mom jokes <eos>\n",
      "Predicted: he was talking about the leader of islamic state in iraq <eos>\n",
      "114m 32s (- 61m 40s) (32500 65%) 3.3121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116m 17s (- 59m 54s) (33000 66%) 3.3534\n",
      "118m 2s (- 58m 8s) (33500 67%) 3.3385\n",
      "119m 47s (- 56m 22s) (34000 68%) 3.3275\n",
      "Input: i been doing so good ... but in and out is calling me . <eos>\n",
      "Target: no . come to the gym . i'll treat u to in n out on friday <eos>\n",
      "Predicted: i hope you are ok . <eos>\n",
      "Input: can u not keep posting pics of or chat bc those icons are very embarrassing <unk> <eos>\n",
      "Target: urs is the best <eos>\n",
      "Predicted: i mean , i thought the same thing . <eos>\n",
      "121m 33s (- 54m 36s) (34500 69%) 3.2980\n",
      "123m 18s (- 52m 50s) (35000 70%) 3.2713\n",
      "125m 3s (- 51m 4s) (35500 71%) 3.2553\n",
      "126m 49s (- 49m 19s) (36000 72%) 3.2585\n",
      "Input: damn man ! you really are obsessed with <unk> .. is your mom as cute as you are ? <eos>\n",
      "Target: my mother hates nazi <unk> <unk> like you worse than i do , thx for asking . <eos>\n",
      "Predicted: i think she is very <unk> . <eos>\n",
      "Input: oh this is all you <eos>\n",
      "Target: so little time ! <eos>\n",
      "Predicted: i am not being <unk> . <eos>\n",
      "128m 34s (- 47m 33s) (36500 73%) 3.2314\n",
      "130m 19s (- 45m 47s) (37000 74%) 3.2766\n",
      "132m 4s (- 44m 1s) (37500 75%) 3.2421\n",
      "133m 49s (- 42m 15s) (38000 76%) 3.2351\n",
      "Input: phew . i was worried people might actually know my real name . the horror . <eos>\n",
      "Target: . also how does one get the title \" leadership expert \" in a <unk> way ? <eos>\n",
      "Predicted: i feel like i need to try it and <unk> for a new <unk> <eos>\n",
      "Input: when mike has <unk> it's awful . sharing for anyone who <unk> . real story from <eos>\n",
      "Target: i am so sorry that mike get them :( <eos>\n",
      "Predicted: please don't give him a credit for him . <eos>\n",
      "135m 34s (- 40m 29s) (38500 77%) 3.1772\n",
      "137m 19s (- 38m 43s) (39000 78%) 3.2018\n",
      "139m 4s (- 36m 58s) (39500 79%) 3.2127\n",
      "140m 49s (- 35m 12s) (40000 80%) 3.1937\n",
      "Input: <unk> , pandering congressional incompetence <eos>\n",
      "Target: we're officially in the <unk> land <eos>\n",
      "Predicted: you should have said it . <eos>\n",
      "Input: and even then , without understanding the why ... <eos>\n",
      "Target: one may be screwing up part of the app without realizing <eos>\n",
      "Predicted: i think this is exactly the correct answer . <eos>\n",
      "142m 34s (- 33m 26s) (40500 81%) 3.1683\n",
      "144m 18s (- 31m 40s) (41000 82%) 3.1781\n",
      "146m 3s (- 29m 55s) (41500 83%) 3.1452\n",
      "147m 49s (- 28m 9s) (42000 84%) 3.1061\n",
      "Input: <unk> problem is he thought giving big business tax breaks would inspire <unk> didn't 12 % int rate <eos>\n",
      "Target: the <unk> of manufacturing to china & mexico killed us and that happened under reagan , bush & clinton . <eos>\n",
      "Predicted: and he supports tpp , <unk> , and <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "Input: . on : <unk> <unk> <unk> <unk> drop in / chat late start but here <eos>\n",
      "Target: thx for visiting & sharing . üòé üë£ üêæ <eos>\n",
      "Predicted: hi <unk> , hi ! please follow me back . thanks for following ! <eos>\n",
      "149m 34s (- 26m 23s) (42500 85%) 3.1158\n",
      "151m 19s (- 24m 37s) (43000 86%) 3.1100\n",
      "153m 4s (- 22m 52s) (43500 87%) 3.0903\n",
      "154m 49s (- 21m 6s) (44000 88%) 3.0941\n",
      "Input: wow so inspirational <eos>\n",
      "Target: if that potato can make it in the big city , so can i <eos>\n",
      "Predicted: you're the best <eos>\n",
      "Input: anyone seen that guy recently ? he know there's an election ? <eos>\n",
      "Target: he's out campaigning for the gop senate races on the dl . <eos>\n",
      "Predicted: no , he didn't . he doesn't have a clue . <eos>\n",
      "156m 34s (- 19m 21s) (44500 89%) 3.0390\n",
      "158m 19s (- 17m 35s) (45000 90%) 3.0271\n",
      "160m 3s (- 15m 49s) (45500 91%) 3.0910\n",
      "161m 48s (- 14m 4s) (46000 92%) 3.0303\n",
      "Input: when you need to do an appropriate protest of your <unk> shoe policy <eos>\n",
      "Target: sweet <unk> <unk> <eos>\n",
      "Predicted: this is the funniest thing i've ever seen <eos>\n",
      "Input: occasionally i have the thought : isn't it crazy we never <unk> found out who sent <unk> after 9/11 ? <eos>\n",
      "Target: oh ... you mean the other terrorist attack on american soil int eh bush era . <eos>\n",
      "Predicted: you should be reaching backwards . <eos>\n",
      "163m 33s (- 12m 18s) (46500 93%) 3.0452\n",
      "165m 18s (- 10m 33s) (47000 94%) 3.0596\n",
      "167m 3s (- 8m 47s) (47500 95%) 3.0377\n",
      "168m 48s (- 7m 2s) (48000 96%) 2.9878\n",
      "Input: it's our turn . <eos>\n",
      "Target: challenge accepted . <eos>\n",
      "Predicted: <unk> ! ! ! <eos>\n",
      "Input: and a <unk> <unk> with digital <unk> , auto & manual controls with wifi <unk> all for $ <unk> <eos>\n",
      "Target: hop to it then <unk> . how hard can it be ? <eos>\n",
      "Predicted: <unk> <unk> & <unk> & <unk> <unk> & <unk> & <unk> & <unk> & <unk> & <unk> <eos>\n",
      "170m 33s (- 5m 16s) (48500 97%) 3.0234\n",
      "172m 18s (- 3m 30s) (49000 98%) 3.0004\n",
      "174m 3s (- 1m 45s) (49500 99%) 2.9819\n",
      "175m 48s (- 0m 0s) (50000 100%) 2.9674\n",
      "Input: i'm mad there's not enough or equal amount of bananas <eos>\n",
      "Target: yea but who eats that many bananas <eos>\n",
      "Predicted: i don't recall that . i don't recall how many people i get rejected . <eos>\n",
      "Input: thanks for the recent follow happy to connect :) have a great tuesday . > > by <eos>\n",
      "Target: thanks ! may you have a great tuesday as well ! üôå üèæ <eos>\n",
      "Predicted: thank you for the jon <unk> ! <eos>\n"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths = random.choice(train_data)\n",
    "\n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        input_batches, input_lengths, target_batches, target_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer\n",
    "    )\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "    \n",
    "    # job.record(epoch, loss)\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "        evaluate_randomly()\n",
    "\n",
    "#     if epoch % plot_every == 0:\n",
    "#         plot_loss_avg = plot_loss_total / plot_every\n",
    "#         plot_losses.append(plot_loss_avg)\n",
    "#         plot_loss_total = 0\n",
    "        \n",
    "#         # TODO: Running average helper\n",
    "#         ecs.append(eca / plot_every)\n",
    "#         dcs.append(dca / plot_every)\n",
    "#         ecs_win = 'encoder grad (%s)' % hostname\n",
    "#         dcs_win = 'decoder grad (%s)' % hostname\n",
    "#         vis.line(np.array(ecs), win=ecs_win, opts={'title': ecs_win})\n",
    "#         vis.line(np.array(dcs), win=dcs_win, opts={'title': dcs_win})\n",
    "#         eca = 0\n",
    "#         dca = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: like this here . <eos>\n",
      "Target: oh , but wait ! <eos>\n",
      "Predicted: wait , what a <unk> of humanity . <eos>\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder.pt')\n",
    "torch.save(decoder.state_dict(), 'decoder.pt')\n",
    "torch.save(embedding.state_dict(), 'embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(corpus.words)\n",
    "embedding2 = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "# Initialize models\n",
    "encoder2 = EncoderRNN(hidden_size, embedding2, n_layers, dropout=dropout)\n",
    "decoder2 = DecoderRNN(hidden_size, vocab_size, embedding2, n_layers, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding2.load_state_dict(torch.load('embedding.pt', map_location=lambda storage, loc: storage))\n",
    "encoder2.load_state_dict(torch.load('encoder.pt', map_location=lambda storage, loc: storage))\n",
    "decoder2.load_state_dict(torch.load('decoder.pt', map_location=lambda storage, loc: storage))\n",
    "\n",
    "if USE_CUDA:\n",
    "    embedding2.cuda()\n",
    "    encoder2.cuda()\n",
    "    decoder2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(encoder2, decoder2, input_seq, max_length=20):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor([input_seq]), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder2.eval()\n",
    "    decoder2.eval()\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder2(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([corpus.word_to_id['<sos>']]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder2(\n",
    "            decoder_input, decoder_hidden\n",
    "        )\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == corpus.word_to_id['<eos>']:\n",
    "            decoded_words.append('<eos>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(corpus.words[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder2.train()\n",
    "    decoder2.train()\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly2():\n",
    "    index = random.choice(range(len(corpus.x_test)))\n",
    "    input_seq, target_seq = corpus.x_test[index], corpus.y_test[index]\n",
    "    output_words = evaluate2(encoder2, decoder2, input_seq)\n",
    "    print(\"Input:\", ' '.join(corpus.ids_to_text(input_seq)))\n",
    "    print(\"Target:\", ' '.join(corpus.ids_to_text(target_seq)))\n",
    "    print(\"Predicted:\", ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: do i have to read art of war to figure out your strategy ! ? <eos>\n",
      "Target: there isn't that i have seen work . many hours of study <eos>\n",
      "Predicted: it is pretty good , it's just a bunch of habit of knowledge . <eos>\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-1.7006e+00  6.8077e-01 -1.2997e+00  ...   1.2223e+00 -4.0719e-01 -7.5291e-02\n",
       " 5.2378e-01  1.8418e-01  1.9792e+00  ...  -2.6599e-02  6.9974e-01  5.8068e-01\n",
       "-1.2809e-02  2.0259e+00 -2.4834e-01  ...   2.0912e-01 -6.8432e-01  7.9243e-02\n",
       "                ...                   ‚ã±                   ...                \n",
       " 5.4996e-01 -4.5863e-02  9.0763e-01  ...   9.0863e-01 -9.7212e-01 -6.7547e-01\n",
       " 2.3864e-01 -9.5131e-01  9.1130e-01  ...   6.0205e-01  7.4683e-01  3.3002e-01\n",
       "-2.3100e-01 -9.5948e-02  5.5557e-01  ...  -7.7474e-01 -1.7721e-01 -5.0797e-01\n",
       "[torch.cuda.FloatTensor of size 10000x500 (GPU 0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
